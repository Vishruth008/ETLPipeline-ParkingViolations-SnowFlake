{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://arrow.apache.org/docs/python/parquet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloads\n",
    "\n",
    "\n",
    "# https://catalog.data.gov/dataset/dof-parking-violation-codes/resource/05e24c51-7b04-4c1b-b70a-ea96996b6835\n",
    "# https://data.cityofnewyork.us/City-Government/Parking-Violations-Issued-Fiscal-Year-2024/pvqr-7yc4/about_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd65470",
   "metadata": {},
   "source": [
    "## Airflow, Docker, Python (Extract) - Initial Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6504d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3364    0  3364    0     0   3125      0 --:--:--  0:00:01 --:--:--  3126\n"
     ]
    }
   ],
   "source": [
    "# !curl 'https://data.cityofnewyork.us/api/views/ncbg-6agr/rows.csv?accessType=DOWNLOAD' -o \"DOF_Parking_Violation_Codes.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf713e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2645M    0 2645M    0     0  4029k      0 --:--:--  0:11:12 --:--:-- 4313k888k  3917k      0 --:--:--  0:00:47 --:--:-- 4244k0     0  3949k      0 --:--:--  0:00:53 --:--:-- 4078k-- 3773k     0 --:--:--  0:01:00 --:--:-- 3773k:-- 4125k    0  3949k      0 --:--:--  0:01:18 --:--:-- 4033k  0 --:--:--  0:02:09 --:--:-- 4385k     0 --:--:--  0:02:15 --:--:-- 4320k0  4020k      0 --:--:--  0:03:04 --:--:-- 4329k --:--:-- 4331k     0 --:--:--  0:03:24 --:--:-- 3909k0:04:17 --:--:-- 3908k-  0:04:32 --:--:-- 4187k --:--:--  0:05:18 --:--:-- 4097k027k      0 --:--:--  0:05:35 --:--:-- 3932k0     0  4020k      0 --:--:--  0:05:58 --:--:-- 4057k   0     0  4011k      0 --:--:--  0:06:46 --:--:-- 4146k  0     0  4027k      0 --:--:--  0:07:07 --:--:-- 4228k0 --:--:--  0:07:33 --:--:-- 3472k  0 --:--:--  0:07:38 --:--:-- 4242k  0  4032k      0 --:--:--  0:08:12 --:--:-- 4059k30k      0 --:--:--  0:08:25 --:--:-- 3876k8:28 --:--:-- 3760k   0  4027k      0 --:--:--  0:08:33 --:--:-- 3943k41M    0     0  4029k      0 --:--:--  0:09:04 --:--:-- 4153k 0     0  4023k      0 --:--:--  0:09:17 --:--:-- 3377k 3978k --:--:-- 3781k      0 --:--:--  0:10:29 --:--:-- 3818k     0 --:--:--  0:10:32 --:--:-- 3772k:10:50 --:--:-- 4311k\n"
     ]
    }
   ],
   "source": [
    "# !curl 'https://data.cityofnewyork.us/api/views/pvqr-7yc4/rows.csv?accessType=DOWNLOAD&api_foundry=true' -o \"Parking_Violations_Issued_-_Fiscal_Year_2024.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6fbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b22f7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/pf1fvjvx2dl42wjkh0j61znm0000gn/T/ipykernel_6869/1492991536.py:10: DtypeWarning: Columns (17,18,20,22,23,29,30,31,32,36,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file read completed in 1.12 minutes\n"
     ]
    }
   ],
   "source": [
    "# Start timing the whole process\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Define file paths\n",
    "csv_path = \"Parking_Violations_Issued_-_Fiscal_Year_2024.csv\"\n",
    "parquet_path = \"Parking_Violations_Issued_-_Fiscal_Year_2024.parquet\"\n",
    "\n",
    "# Time for reading CSV file\n",
    "read_start_time = time.time()\n",
    "df = pd.read_csv(csv_path)\n",
    "read_end_time = time.time()\n",
    "read_time_taken = (read_end_time - read_start_time) / 60\n",
    "print(f\"CSV file read completed in {read_time_taken:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9b99b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted object columns to string\n",
      "Converted DataFrame to PyArrow Table\n"
     ]
    }
   ],
   "source": [
    "# Convert object columns to string\n",
    "obj_cols = df.select_dtypes(include=['object']).columns\n",
    "df[obj_cols] = df[obj_cols].astype(str)\n",
    "print(\"Converted object columns to string\")\n",
    "\n",
    "# Convert DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(df)\n",
    "print(\"Converted DataFrame to PyArrow Table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cef500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file write completed in 0.29 minutes\n",
      "Total time taken: 5.01 minutes\n"
     ]
    }
   ],
   "source": [
    "# Time for writing to Parquet file\n",
    "write_start_time = time.time()\n",
    "pq.write_table(table, parquet_path)\n",
    "write_end_time = time.time()\n",
    "write_time_taken = (write_end_time - write_start_time) / 60\n",
    "print(f\"Parquet file write completed in {write_time_taken:.2f} minutes\")\n",
    "\n",
    "# End timing the whole process\n",
    "total_end_time = time.time()\n",
    "total_time_taken = (total_end_time - total_start_time) / 60\n",
    "\n",
    "print(f\"Total time taken: {total_time_taken:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c3b407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "DATA_FOLDER='tt'\n",
    "urls = {\n",
    "        \"DOF_Parking_Violation_Codes.csv\": 'https://data.cityofnewyork.us/api/views/ncbg-6agr/rows.csv?accessType=DOWNLOAD',\n",
    "        \"Parking_Violations_Issued_Fiscal_Year_2024.csv\": 'https://data.cityofnewyork.us/api/views/pvqr-7yc4/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "}\n",
    "for filename, url in urls.items():\n",
    "    local_file_path = os.path.join(DATA_FOLDER, filename)\n",
    "\n",
    "    # Start the request and get the total length of the content\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_length = response.headers.get('content-length')\n",
    "\n",
    "    if total_length is None:\n",
    "        # No content length header\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} locally at {local_file_path}\")\n",
    "    else:\n",
    "        total_length = int(total_length)\n",
    "        # Download the file with a progress bar\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in tqdm(response.iter_content(chunk_size=1024), total=total_length // 1024, unit='KB'):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} locally at {local_file_path}\")\n",
    "        \n",
    "parking_violations_df=pd.read_csv(\"Parking_Violations_Issued_-_Fiscal_Year_2024.csv\")\n",
    "parking_violations_df.shape\n",
    "df_percent = parking_violations_df.sample(frac=0.12)\n",
    "df_percent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe73f2",
   "metadata": {},
   "source": [
    "## Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4813b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet file read completed in 0.22 minutes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "# Time for reading Parquet file\n",
    "read_start_time = time.time()\n",
    "parking_codes_df=pd.read_csv(\"DOF_Parking_Violation_Codes.csv\")\n",
    "parking_violations_df=pd.read_parquet(\"Parking_Violations_Issued_-_Fiscal_Year_2024.parquet\")\n",
    "read_end_time = time.time()\n",
    "read_time_taken = (read_end_time - read_start_time) / 60\n",
    "print(f\"Parquet file read completed in {read_time_taken:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd9dd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 4 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   CODE                         97 non-null     int64 \n",
      " 1   DEFINITION                   97 non-null     object\n",
      " 2   ManhattanÂ  96th St. & below  97 non-null     int64 \n",
      " 3   All Other Areas              97 non-null     int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "parking_codes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27622d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14765377 entries, 0 to 14765376\n",
      "Data columns (total 43 columns):\n",
      " #   Column                             Dtype  \n",
      "---  ------                             -----  \n",
      " 0   Summons Number                     int64  \n",
      " 1   Plate ID                           object \n",
      " 2   Registration State                 object \n",
      " 3   Plate Type                         object \n",
      " 4   Issue Date                         object \n",
      " 5   Violation Code                     int64  \n",
      " 6   Vehicle Body Type                  object \n",
      " 7   Vehicle Make                       object \n",
      " 8   Issuing Agency                     object \n",
      " 9   Street Code1                       int64  \n",
      " 10  Street Code2                       int64  \n",
      " 11  Street Code3                       int64  \n",
      " 12  Vehicle Expiration Date            int64  \n",
      " 13  Violation Location                 float64\n",
      " 14  Violation Precinct                 int64  \n",
      " 15  Issuer Precinct                    int64  \n",
      " 16  Issuer Code                        int64  \n",
      " 17  Issuer Command                     object \n",
      " 18  Issuer Squad                       object \n",
      " 19  Violation Time                     object \n",
      " 20  Time First Observed                object \n",
      " 21  Violation County                   object \n",
      " 22  Violation In Front Of Or Opposite  object \n",
      " 23  House Number                       object \n",
      " 24  Street Name                        object \n",
      " 25  Intersecting Street                object \n",
      " 26  Date First Observed                int64  \n",
      " 27  Law Section                        int64  \n",
      " 28  Sub Division                       object \n",
      " 29  Violation Legal Code               object \n",
      " 30  Days Parking In Effect             object \n",
      " 31  From Hours In Effect               object \n",
      " 32  To Hours In Effect                 object \n",
      " 33  Vehicle Color                      object \n",
      " 34  Unregistered Vehicle?              float64\n",
      " 35  Vehicle Year                       int64  \n",
      " 36  Meter Number                       object \n",
      " 37  Feet From Curb                     int64  \n",
      " 38  Violation Post Code                object \n",
      " 39  Violation Description              object \n",
      " 40  No Standing or Stopping Violation  float64\n",
      " 41  Hydrant Violation                  float64\n",
      " 42  Double Parking Violation           float64\n",
      "dtypes: float64(5), int64(13), object(25)\n",
      "memory usage: 4.7+ GB\n"
     ]
    }
   ],
   "source": [
    "parking_violations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15868474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632595a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/ishaan45/csv-to-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7f5ca",
   "metadata": {},
   "source": [
    "## Airflow Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws_default\n",
    "\n",
    "# AWS_ACCESS_KEY_ID=\n",
    "# AWS_SECRET_ACCESS_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a712638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snowflake_default\n",
    "\n",
    "# PARKING_SCHEMA\n",
    "\n",
    "# badreeshshetty\n",
    "\n",
    "# \n",
    "\n",
    "# PARKING_WAREHOUSE\n",
    "\n",
    "# PARKING_DB\n",
    "\n",
    "# ACCOUNTADMIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688c67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parking_violations_data(year):\n",
    "    parking_violations_data = {\n",
    "        \"2014\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2014\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/jt7v-77mi/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2015\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2015\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/c284-tqph/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2016\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2016\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/kiv2-tbus/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2017\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2017\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/2bnn-yakx/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2018\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2018\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/a5td-mswe/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2019\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2019\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/faiq-9dfq/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2020\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2020\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/p7t3-5i9s/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2021\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2021\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/kvfd-bves/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2022\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2022\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/7mxj-7a6y/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2023\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2023\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/869v-vr48/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        },\n",
    "        \"2024\": {\n",
    "            \"file_name\": \"Parking_Violations_Issued_Fiscal_Year_2024\",\n",
    "            \"url\": 'https://data.cityofnewyork.us/api/views/pvqr-7yc4/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return parking_violations_data.get(str(year), \"Invalid year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e38701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parking_Violations_Issued_Fiscal_Year_2023'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parking Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d01c390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parking_violations_file_name:  Parking_Violations_Issued_Fiscal_Year_2019\n",
      "parking_violations_url:  https://data.cityofnewyork.us/api/views/faiq-9dfq/rows.csv?accessType=DOWNLOAD&api_foundry=true\n"
     ]
    }
   ],
   "source": [
    "year=2019\n",
    "data=get_parking_violations_data(year)\n",
    "\n",
    "print(\"parking_violations_file_name: \",data[\"file_name\"])\n",
    "print(\"parking_violations_url: \",data[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b374df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9ffd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8fbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091b8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223608e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a14e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# search_str = 'o=[i(\"manifest\",\"manifest.json\"),i(\"catalog\",\"catalog.json\")]'\n",
    "\n",
    "# with open('target/index.html', 'r') as f:\n",
    "#     content_index = f.read()\n",
    "    \n",
    "# with open('target/manifest.json', 'r') as f:\n",
    "#     json_manifest = json.loads(f.read())\n",
    "\n",
    "# with open('target/catalog.json', 'r') as f:\n",
    "#     json_catalog = json.loads(f.read())\n",
    "    \n",
    "# with open('target/dbt_docs_parking_violations.html', 'w') as f:\n",
    "#     new_str = \"o=[{label: 'manifest', data: \"+json.dumps(json_manifest)+\"},{label: 'catalog', data: \"+json.dumps(json_catalog)+\"}]\"\n",
    "#     new_content = content_index.replace(search_str, new_str)\n",
    "#     f.write(new_content)\n",
    "    \n",
    "    \n",
    "# python3 -m http.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6858cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e8c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmail-parking-violations-ny-dae\n",
    "# utzo enud uvno kqoy\n",
    "# G App Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be99198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parking_violations_file_name\n",
    "\n",
    "# parking_violations_url\n",
    " \n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2014\": 'https://data.cityofnewyork.us/api/views/jt7v-77mi/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2015\": 'https://data.cityofnewyork.us/api/views/c284-tqph/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2016\": 'https://data.cityofnewyork.us/api/views/kiv2-tbus/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2017\": 'https://data.cityofnewyork.us/api/views/2bnn-yakx/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2018\": 'https://data.cityofnewyork.us/api/views/a5td-mswe/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2019\": 'https://data.cityofnewyork.us/api/views/faiq-9dfq/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2020\": 'https://data.cityofnewyork.us/api/views/p7t3-5i9s/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2021\": 'https://data.cityofnewyork.us/api/views/kvfd-bves/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2022\": 'https://data.cityofnewyork.us/api/views/7mxj-7a6y/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2023\": 'https://data.cityofnewyork.us/api/views/869v-vr48/rows.csv?accessType=DOWNLOAD&api_foundry=true',\n",
    "\n",
    "# \"Parking_Violations_Issued_Fiscal_Year_2024\": 'https://data.cityofnewyork.us/api/views/pvqr-7yc4/rows.csv?accessType=DOWNLOAD&api_foundry=true'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79849b8d",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effb0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# SNOWFLAKE_ACCOUNT = ''\n",
    "# SNOWFLAKE_USER = ''\n",
    "# SNOWFLAKE_PASSWORD = ''\n",
    "SNOWFLAKE_WAREHOUSE = 'PARKING_WAREHOUSE'\n",
    "SNOWFLAKE_DB = 'PARKING_DB'\n",
    "SNOWFLAKE_SCHEMA = 'PARKING_SCHEMA'\n",
    "SNOWFLAKE_TABLE_PARKING_VIOLATIONS = 'PARKING_VIOLATIONS'\n",
    "SNOWFLAKE_TABLE_PARKING_VIOLATIONS_CODE = 'PARKING_VIOLATIONS_CODE'\n",
    "PARQUET_FILE_PATH = 'Parking_Violations_Issued_-_Fiscal_Year_2024.parquet'\n",
    "# CSV_FILE_PATH = '<your_csv_file_path>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15aa3891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Establish a connection to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    "    warehouse=SNOWFLAKE_WAREHOUSE,\n",
    "    database=SNOWFLAKE_DB,\n",
    "    schema=SNOWFLAKE_SCHEMA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25cc77ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully to Snowflake.\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Establish a connection to Snowflake\n",
    "conn = snowflake.connector.connect(\n",
    "    user=SNOWFLAKE_USER,\n",
    "    password=SNOWFLAKE_PASSWORD,\n",
    "    account=SNOWFLAKE_ACCOUNT,\n",
    "    warehouse=SNOWFLAKE_WAREHOUSE\n",
    ")\n",
    "\n",
    "conn.cursor().execute(f\"CREATE WAREHOUSE IF NOT EXISTS {SNOWFLAKE_WAREHOUSE} WITH WAREHOUSE_SIZE='x-small'\")\n",
    "conn.cursor().execute(f\"CREATE DATABASE IF NOT EXISTS {SNOWFLAKE_DB}\")\n",
    "conn.cursor().execute(f\"CREATE SCHEMA IF NOT EXISTS {SNOWFLAKE_SCHEMA}\")\n",
    "\n",
    "# Use the specified database and schema\n",
    "conn.cursor().execute(f\"USE DATABASE {SNOWFLAKE_DB}\")\n",
    "conn.cursor().execute(f\"USE SCHEMA {SNOWFLAKE_SCHEMA}\")\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df = pd.read_parquet(PARQUET_FILE_PATH)\n",
    "\n",
    "# Modify column names: capitalize and remove spaces\n",
    "df.columns = [col.upper().replace(' ', '_') for col in df.columns]\n",
    "df.columns = [col.upper().replace('?', '') for col in df.columns]\n",
    "\n",
    "# Map DataFrame column types to Snowflake data types\n",
    "type_mapping = {\n",
    "    'object': 'STRING',\n",
    "    'int64': 'NUMBER',\n",
    "    'float64': 'FLOAT',\n",
    "    'bool': 'BOOLEAN',\n",
    "    'datetime64[ns]': 'TIMESTAMP'\n",
    "}\n",
    "\n",
    "# Get column names and types from the DataFrame\n",
    "columns = ', '.join([f\"{col} {type_mapping[str(dtype)]}\" for col, dtype in df.dtypes.items()])\n",
    "\n",
    "# Create the table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {SNOWFLAKE_TABLE_PARKING_VIOLATIONS} (\n",
    "    {columns}\n",
    ")\n",
    "\"\"\"\n",
    "conn.cursor().execute(create_table_query)\n",
    "\n",
    "# Create a temporary stage\n",
    "temp_stage_name = 'TEMP_STAGE_PARQUET'\n",
    "conn.cursor().execute(f\"CREATE OR REPLACE TEMPORARY STAGE {temp_stage_name}\")\n",
    "\n",
    "# # Write the DataFrame to a Parquet file in the temporary stage\n",
    "temp_parquet_file_path = 'Parking_Violations_Issued_-_Fiscal_Year_2024.parquet'\n",
    "# df.to_parquet(temp_parquet_file_path)\n",
    "\n",
    "# Upload the Parquet file to the temporary stage\n",
    "conn.cursor().execute(f\"PUT file://{temp_parquet_file_path} @{temp_stage_name}\")\n",
    "\n",
    "# Copy the data from the temporary stage to the Snowflake table\n",
    "copy_query = f\"\"\"\n",
    "COPY INTO {SNOWFLAKE_TABLE_PARKING_VIOLATIONS}\n",
    "FROM @{temp_stage_name}/Parking_Violations_Issued_-_Fiscal_Year_2024.parquet\n",
    "FILE_FORMAT = (TYPE = PARQUET)\n",
    "MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n",
    "\"\"\"\n",
    "conn.cursor().execute(copy_query)\n",
    "\n",
    "# Clean up: remove the temporary stage\n",
    "conn.cursor().execute(f\"REMOVE @{temp_stage_name}\")\n",
    "conn.cursor().execute(f\"DROP STAGE {temp_stage_name}\")\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"Data loaded successfully to Snowflake.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3e58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd44c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c249a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1be5ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/pf1fvjvx2dl42wjkh0j61znm0000gn/T/ipykernel_19862/4241511122.py:25: DtypeWarning: Columns (17,18,20,22,23,29,30,31,32,36,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(io.StringIO(csv_content))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Summons Number  Plate ID Registration State Plate Type  Issue Date  \\\n",
      "0             1159637337   KZH2758                 NY        PAS  06/09/2023   \n",
      "1             1252960645   JPD8746                 NY        PAS  06/30/2023   \n",
      "2             1252960669   JPD8746                 NY        PAS  06/30/2023   \n",
      "3             1252994126   MBH9245                 99        PAS  07/06/2023   \n",
      "4             1252994175   MBH9245                 PA        PAS  07/08/2023   \n",
      "...                  ...       ...                ...        ...         ...   \n",
      "14765372      9133935178   LEK7929                 NY        PAS  05/28/2024   \n",
      "14765373      9133935180   LJF1533                 NY        PAS  05/28/2024   \n",
      "14765374      9133935191     MAL1K                 NY        OMT  05/28/2024   \n",
      "14765375      9133935208  T720318C                 NY        OMT  05/28/2024   \n",
      "14765376      9133935210   LBJ4302                 NY        PAS  05/28/2024   \n",
      "\n",
      "          Violation Code Vehicle Body Type Vehicle Make Issuing Agency  \\\n",
      "0                     67               VAN        HONDA              P   \n",
      "1                     87              SUBN        LINCO              M   \n",
      "2                     31              SUBN        LINCO              M   \n",
      "3                     20               SDN          KIA              M   \n",
      "4                     40               SDN          KIA              M   \n",
      "...                  ...               ...          ...            ...   \n",
      "14765372              14              4DSD        LEXUS              T   \n",
      "14765373              14              4DSD        TOYOT              T   \n",
      "14765374              46              SUBN        TOYOT              T   \n",
      "14765375              46              4DSD        TOYOT              T   \n",
      "14765376              46              SUBN        HONDA              T   \n",
      "\n",
      "          Street Code1  ...  Vehicle Color  Unregistered Vehicle?  \\\n",
      "0                    0  ...           BLUE                    0.0   \n",
      "1                17870  ...           GRAY                    0.0   \n",
      "2                17870  ...           GRAY                    0.0   \n",
      "3                12690  ...          WHITE                    0.0   \n",
      "4                12690  ...          WHITE                    0.0   \n",
      "...                ...  ...            ...                    ...   \n",
      "14765372         67530  ...            DKG                    NaN   \n",
      "14765373         22680  ...             GR                    NaN   \n",
      "14765374         22680  ...             GY                    NaN   \n",
      "14765375         22680  ...             GY                    NaN   \n",
      "14765376         28830  ...             BK                    NaN   \n",
      "\n",
      "          Vehicle Year  Meter Number  Feet From Curb  Violation Post Code  \\\n",
      "0                 2006             -               0                  NaN   \n",
      "1                 2020             -               0                  NaN   \n",
      "2                 2020             -               0                  NaN   \n",
      "3                    0             -               0                  NaN   \n",
      "4                    0             -               0                  NaN   \n",
      "...                ...           ...             ...                  ...   \n",
      "14765372          2014           NaN               0                  CC1   \n",
      "14765373          2016           NaN               0                  CC1   \n",
      "14765374          2016           NaN               0                  CC1   \n",
      "14765375          2016           NaN               0                  CC1   \n",
      "14765376          2021           NaN               0                  CC1   \n",
      "\n",
      "                 Violation Description No Standing or Stopping Violation  \\\n",
      "0                                  NaN                               NaN   \n",
      "1                                  NaN                               NaN   \n",
      "2                                  NaN                               NaN   \n",
      "3                                  NaN                               NaN   \n",
      "4                                  NaN                               NaN   \n",
      "...                                ...                               ...   \n",
      "14765372                14-No Standing                               NaN   \n",
      "14765373                14-No Standing                               NaN   \n",
      "14765374  46A-Double Parking (Non-COM)                               NaN   \n",
      "14765375  46A-Double Parking (Non-COM)                               NaN   \n",
      "14765376  46A-Double Parking (Non-COM)                               NaN   \n",
      "\n",
      "         Hydrant Violation Double Parking Violation  \n",
      "0                      NaN                      NaN  \n",
      "1                      NaN                      NaN  \n",
      "2                      NaN                      NaN  \n",
      "3                      NaN                      NaN  \n",
      "4                      NaN                      NaN  \n",
      "...                    ...                      ...  \n",
      "14765372               NaN                      NaN  \n",
      "14765373               NaN                      NaN  \n",
      "14765374               NaN                      NaN  \n",
      "14765375               NaN                      NaN  \n",
      "14765376               NaN                      NaN  \n",
      "\n",
      "[14765377 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import gzip \n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "S3_BUCKET = 'parking-violations-bucket'\n",
    "CSV_FILE_PATH = 'Parking_Violations_Issued_Fiscal_Year_2024.csv.gz'\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=\"AKIA3FLDYXQMIJKM4EOJ\",\n",
    "    aws_secret_access_key=\"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\",\n",
    ")\n",
    "\n",
    "# Read the gzipped CSV content from S3\n",
    "response = s3_client.get_object(Bucket=S3_BUCKET, Key=CSV_FILE_PATH)\n",
    "gzipped_csv_content = response['Body'].read()\n",
    "\n",
    "# Decompress the gzipped content\n",
    "with gzip.GzipFile(fileobj=io.BytesIO(gzipped_csv_content), mode='rb') as gz:\n",
    "    csv_content = gz.read().decode('utf-8')\n",
    "\n",
    "# Read the CSV content into a DataFrame\n",
    "df = pd.read_csv(io.StringIO(csv_content))\n",
    "\n",
    "# Print the DataFrame to verify\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7740c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc866ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1\n",
      "Processing chunk 2\n",
      "Processing chunk 3\n",
      "Processing chunk 4\n",
      "Processing chunk 5\n",
      "Processing chunk 6\n",
      "Processing chunk 7\n",
      "Processing chunk 8\n",
      "Processing chunk 9\n",
      "Processing chunk 10\n",
      "Processing chunk 11\n",
      "Processing chunk 12\n",
      "Processing chunk 13\n",
      "Processing chunk 14\n",
      "Processing chunk 15\n",
      "Processing chunk 16\n",
      "Processing chunk 17\n",
      "Processing chunk 18\n",
      "Processing chunk 19\n",
      "Processing chunk 20\n",
      "Processing chunk 21\n",
      "Processing chunk 22\n",
      "Processing chunk 23\n",
      "Processing chunk 24\n",
      "Processing chunk 25\n",
      "Processing chunk 26\n",
      "Processing chunk 27\n",
      "Processing chunk 28\n",
      "Processing chunk 29\n",
      "Processing chunk 30\n",
      "Processing chunk 31\n",
      "Processing chunk 32\n",
      "Processing chunk 33\n",
      "Processing chunk 34\n",
      "Processing chunk 35\n",
      "Processing chunk 36\n",
      "Processing chunk 37\n",
      "Processing chunk 38\n",
      "Processing chunk 39\n",
      "Processing chunk 40\n",
      "Processing chunk 41\n",
      "Processing chunk 42\n",
      "Processing chunk 43\n",
      "Processing chunk 44\n",
      "Processing chunk 45\n",
      "Processing chunk 46\n",
      "Processing chunk 47\n",
      "Processing chunk 48\n",
      "Processing chunk 49\n",
      "Processing chunk 50\n",
      "Processing chunk 51\n",
      "Processing chunk 52\n",
      "Processing chunk 53\n",
      "Processing chunk 54\n",
      "Processing chunk 55\n",
      "Processing chunk 56\n",
      "Processing chunk 57\n",
      "Processing chunk 58\n",
      "Processing chunk 59\n",
      "Processing chunk 60\n",
      "Processing chunk 61\n",
      "Processing chunk 62\n",
      "Processing chunk 63\n",
      "Processing chunk 64\n",
      "Processing chunk 65\n",
      "Processing chunk 66\n",
      "Processing chunk 67\n",
      "Processing chunk 68\n",
      "Processing chunk 69\n",
      "Processing chunk 70\n",
      "Processing chunk 71\n",
      "Processing chunk 72\n",
      "Processing chunk 73\n",
      "Processing chunk 74\n",
      "Processing chunk 75\n",
      "Processing chunk 76\n",
      "Processing chunk 77\n",
      "Processing chunk 78\n",
      "Processing chunk 79\n",
      "Processing chunk 80\n",
      "Processing chunk 81\n",
      "Processing chunk 82\n",
      "Processing chunk 83\n",
      "Processing chunk 84\n",
      "Processing chunk 85\n",
      "Processing chunk 86\n",
      "Processing chunk 87\n",
      "Processing chunk 88\n",
      "Processing chunk 89\n",
      "Processing chunk 90\n",
      "Processing chunk 91\n",
      "Processing chunk 92\n",
      "Processing chunk 93\n",
      "Processing chunk 94\n",
      "Processing chunk 95\n",
      "Processing chunk 96\n",
      "Processing chunk 97\n",
      "Processing chunk 98\n",
      "Processing chunk 99\n",
      "Processing chunk 100\n",
      "Processing chunk 101\n",
      "Processing chunk 102\n",
      "Processing chunk 103\n",
      "Processing chunk 104\n",
      "Processing chunk 105\n",
      "Processing chunk 106\n",
      "Processing chunk 107\n",
      "Processing chunk 108\n",
      "Processing chunk 109\n",
      "Processing chunk 110\n",
      "Processing chunk 111\n",
      "Processing chunk 112\n",
      "Processing chunk 113\n",
      "Processing chunk 114\n",
      "Processing chunk 115\n",
      "Processing chunk 116\n",
      "Processing chunk 117\n",
      "Processing chunk 118\n",
      "Processing chunk 119\n",
      "Processing chunk 120\n",
      "Processing chunk 121\n",
      "Processing chunk 122\n",
      "Processing chunk 123\n",
      "Processing chunk 124\n",
      "Processing chunk 125\n",
      "Processing chunk 126\n",
      "Processing chunk 127\n",
      "Processing chunk 128\n",
      "Processing chunk 129\n",
      "Processing chunk 130\n",
      "Processing chunk 131\n",
      "Processing chunk 132\n",
      "Processing chunk 133\n",
      "Processing chunk 134\n",
      "Processing chunk 135\n",
      "Processing chunk 136\n",
      "Processing chunk 137\n",
      "Processing chunk 138\n",
      "Processing chunk 139\n",
      "Processing chunk 140\n",
      "Processing chunk 141\n",
      "Processing chunk 142\n",
      "Processing chunk 143\n",
      "Processing chunk 144\n",
      "Processing chunk 145\n",
      "Processing chunk 146\n",
      "Processing chunk 147\n",
      "Processing chunk 148\n",
      "Processing chunk 149\n",
      "Processing chunk 150\n",
      "Processing chunk 151\n",
      "Processing chunk 152\n",
      "Processing chunk 153\n",
      "Processing chunk 154\n",
      "Processing chunk 155\n",
      "Processing chunk 156\n",
      "Processing chunk 157\n",
      "Processing chunk 158\n",
      "Processing chunk 159\n",
      "Processing chunk 160\n",
      "Processing chunk 161\n",
      "Processing chunk 162\n",
      "Processing chunk 163\n",
      "Processing chunk 164\n",
      "Processing chunk 165\n",
      "Processing chunk 166\n",
      "Processing chunk 167\n",
      "Processing chunk 168\n",
      "Processing chunk 169\n",
      "Processing chunk 170\n",
      "Processing chunk 171\n",
      "Processing chunk 172\n",
      "Processing chunk 173\n",
      "Processing chunk 174\n",
      "Processing chunk 175\n",
      "Processing chunk 176\n",
      "Processing chunk 177\n",
      "Processing chunk 178\n",
      "Processing chunk 179\n",
      "Processing chunk 180\n",
      "Processing chunk 181\n",
      "Processing chunk 182\n",
      "Processing chunk 183\n",
      "Processing chunk 184\n",
      "Processing chunk 185\n",
      "Processing chunk 186\n",
      "Processing chunk 187\n",
      "Processing chunk 188\n",
      "Processing chunk 189\n",
      "Processing chunk 190\n",
      "Processing chunk 191\n",
      "Processing chunk 192\n",
      "Processing chunk 193\n",
      "Processing chunk 194\n",
      "Processing chunk 195\n",
      "Processing chunk 196\n",
      "Processing chunk 197\n",
      "Processing chunk 198\n",
      "Processing chunk 199\n",
      "Processing chunk 200\n",
      "Processing chunk 201\n",
      "Processing chunk 202\n",
      "Processing chunk 203\n",
      "Processing chunk 204\n",
      "Processing chunk 205\n",
      "Processing chunk 206\n",
      "Processing chunk 207\n",
      "Processing chunk 208\n",
      "Processing chunk 209\n",
      "Processing chunk 210\n",
      "Processing chunk 211\n",
      "Processing chunk 212\n",
      "Processing chunk 213\n",
      "Processing chunk 214\n",
      "Processing chunk 215\n",
      "Processing chunk 216\n",
      "Processing chunk 217\n",
      "Processing chunk 218\n",
      "Processing chunk 219\n",
      "Processing chunk 220\n",
      "Processing chunk 221\n",
      "Processing chunk 222\n",
      "Processing chunk 223\n",
      "Processing chunk 224\n",
      "Processing chunk 225\n",
      "Processing chunk 226\n",
      "Processing chunk 227\n",
      "Processing chunk 228\n",
      "Processing chunk 229\n",
      "Processing chunk 230\n",
      "Processing chunk 231\n",
      "Processing chunk 232\n",
      "Processing chunk 233\n",
      "Processing chunk 234\n",
      "Processing chunk 235\n",
      "Processing chunk 236\n",
      "Processing chunk 237\n",
      "Processing chunk 238\n",
      "Processing chunk 239\n",
      "Processing chunk 240\n",
      "Processing chunk 241\n",
      "Processing chunk 242\n",
      "Processing chunk 243\n",
      "Processing chunk 244\n",
      "Processing chunk 245\n",
      "Processing chunk 246\n",
      "Processing chunk 247\n",
      "Processing chunk 248\n",
      "Processing chunk 249\n",
      "Processing chunk 250\n",
      "Processing chunk 251\n",
      "Processing chunk 252\n",
      "Processing chunk 253\n",
      "Processing chunk 254\n",
      "Processing chunk 255\n",
      "Processing chunk 256\n",
      "Processing chunk 257\n",
      "Processing chunk 258\n",
      "Processing chunk 259\n",
      "Processing chunk 260\n",
      "Processing chunk 261\n",
      "Processing chunk 262\n",
      "Processing chunk 263\n",
      "Processing chunk 264\n",
      "Processing chunk 265\n",
      "Processing chunk 266\n",
      "Processing chunk 267\n",
      "Processing chunk 268\n",
      "Processing chunk 269\n",
      "Processing chunk 270\n",
      "Processing chunk 271\n",
      "Processing chunk 272\n",
      "Processing chunk 273\n",
      "Processing chunk 274\n",
      "Processing chunk 275\n",
      "Processing chunk 276\n",
      "Processing chunk 277\n",
      "Processing chunk 278\n",
      "Processing chunk 279\n",
      "Processing chunk 280\n",
      "Processing chunk 281\n",
      "Processing chunk 282\n",
      "Processing chunk 283\n",
      "Processing chunk 284\n",
      "Processing chunk 285\n",
      "Processing chunk 286\n",
      "Processing chunk 287\n",
      "Processing chunk 288\n",
      "Processing chunk 289\n",
      "Processing chunk 290\n",
      "Processing chunk 291\n",
      "Processing chunk 292\n",
      "Processing chunk 293\n",
      "Processing chunk 294\n",
      "Processing chunk 295\n",
      "Processing chunk 296\n",
      "Processing chunk 297\n",
      "Processing chunk 298\n",
      "Processing chunk 299\n",
      "Processing chunk 300\n",
      "Processing chunk 301\n",
      "Processing chunk 302\n",
      "Processing chunk 303\n",
      "Processing chunk 304\n",
      "Processing chunk 305\n",
      "Processing chunk 306\n",
      "Processing chunk 307\n",
      "Processing chunk 308\n",
      "Processing chunk 309\n",
      "Processing chunk 310\n",
      "Processing chunk 311\n",
      "Processing chunk 312\n",
      "Processing chunk 313\n",
      "Processing chunk 314\n",
      "Processing chunk 315\n",
      "Processing chunk 316\n",
      "Processing chunk 317\n",
      "Processing chunk 318\n",
      "Processing chunk 319\n",
      "Processing chunk 320\n",
      "Processing chunk 321\n",
      "Processing chunk 322\n",
      "Processing chunk 323\n",
      "Processing chunk 324\n",
      "Processing chunk 325\n",
      "Processing chunk 326\n",
      "Processing chunk 327\n",
      "Processing chunk 328\n",
      "Processing chunk 329\n",
      "Processing chunk 330\n",
      "Processing chunk 331\n",
      "Processing chunk 332\n",
      "Processing chunk 333\n",
      "Processing chunk 334\n",
      "Processing chunk 335\n",
      "Processing chunk 336\n",
      "Processing chunk 337\n",
      "Processing chunk 338\n",
      "Processing chunk 339\n",
      "Processing chunk 340\n",
      "Processing chunk 341\n",
      "Processing chunk 342\n",
      "Processing chunk 343\n",
      "Processing chunk 344\n",
      "Processing chunk 345\n",
      "Processing chunk 346\n",
      "Processing chunk 347\n",
      "Processing chunk 348\n",
      "Processing chunk 349\n",
      "Processing chunk 350\n",
      "Processing chunk 351\n",
      "Processing chunk 352\n",
      "Processing chunk 353\n",
      "Processing chunk 354\n",
      "Processing chunk 355\n",
      "Processing chunk 356\n",
      "Processing chunk 357\n",
      "Processing chunk 358\n",
      "Processing chunk 359\n",
      "Processing chunk 360\n",
      "Processing chunk 361\n",
      "Processing chunk 362\n",
      "Processing chunk 363\n",
      "Processing chunk 364\n",
      "Processing chunk 365\n",
      "Processing chunk 366\n",
      "Processing chunk 367\n",
      "Processing chunk 368\n",
      "Processing chunk 369\n",
      "Processing chunk 370\n",
      "Processing chunk 371\n",
      "Processing chunk 372\n",
      "Processing chunk 373\n",
      "Processing chunk 374\n",
      "Processing chunk 375\n",
      "Processing chunk 376\n",
      "Processing chunk 377\n",
      "Processing chunk 378\n",
      "Processing chunk 379\n",
      "Processing chunk 380\n",
      "Processing chunk 381\n",
      "Processing chunk 382\n",
      "Processing chunk 383\n",
      "Processing chunk 384\n",
      "Processing chunk 385\n",
      "Processing chunk 386\n",
      "Processing chunk 387\n",
      "Processing chunk 388\n",
      "Processing chunk 389\n",
      "Processing chunk 390\n",
      "Processing chunk 391\n",
      "Processing chunk 392\n",
      "Processing chunk 393\n",
      "Processing chunk 394\n",
      "Processing chunk 395\n",
      "Processing chunk 396\n",
      "Processing chunk 397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 398\n",
      "Processing chunk 399\n",
      "Processing chunk 400\n",
      "Processing chunk 401\n",
      "Processing chunk 402\n",
      "Processing chunk 403\n",
      "Processing chunk 404\n",
      "Processing chunk 405\n",
      "Processing chunk 406\n",
      "Processing chunk 407\n",
      "Processing chunk 408\n",
      "Processing chunk 409\n",
      "Processing chunk 410\n",
      "Processing chunk 411\n",
      "Processing chunk 412\n",
      "Processing chunk 413\n",
      "Processing chunk 414\n",
      "Processing chunk 415\n",
      "Processing chunk 416\n",
      "Processing chunk 417\n",
      "Processing chunk 418\n",
      "Processing chunk 419\n",
      "Processing chunk 420\n",
      "Processing chunk 421\n",
      "Processing chunk 422\n",
      "Processing chunk 423\n",
      "Processing chunk 424\n",
      "Processing chunk 425\n",
      "Processing chunk 426\n",
      "Processing chunk 427\n",
      "Processing chunk 428\n",
      "Processing chunk 429\n",
      "Processing chunk 430\n",
      "Processing chunk 431\n",
      "Processing chunk 432\n",
      "Processing chunk 433\n",
      "Processing chunk 434\n",
      "Processing chunk 435\n",
      "Processing chunk 436\n",
      "Processing chunk 437\n",
      "Processing chunk 438\n",
      "Processing chunk 439\n",
      "Processing chunk 440\n",
      "Processing chunk 441\n",
      "Processing chunk 442\n",
      "Processing chunk 443\n",
      "Processing chunk 444\n",
      "Processing chunk 445\n",
      "Processing chunk 446\n",
      "Processing chunk 447\n",
      "Processing chunk 448\n",
      "Processing chunk 449\n",
      "Processing chunk 450\n",
      "Processing chunk 451\n",
      "Processing chunk 452\n",
      "Processing chunk 453\n",
      "Processing chunk 454\n",
      "Processing chunk 455\n",
      "Processing chunk 456\n",
      "Processing chunk 457\n",
      "Processing chunk 458\n",
      "Processing chunk 459\n",
      "Processing chunk 460\n",
      "Processing chunk 461\n",
      "Processing chunk 462\n",
      "Processing chunk 463\n",
      "Processing chunk 464\n",
      "Processing chunk 465\n",
      "Processing chunk 466\n",
      "Processing chunk 467\n",
      "Processing chunk 468\n",
      "Processing chunk 469\n",
      "Processing chunk 470\n",
      "Processing chunk 471\n",
      "Processing chunk 472\n",
      "Processing chunk 473\n",
      "Processing chunk 474\n",
      "Processing chunk 475\n",
      "Processing chunk 476\n",
      "Processing chunk 477\n",
      "Processing chunk 478\n",
      "Processing chunk 479\n",
      "Processing chunk 480\n",
      "Processing chunk 481\n",
      "Processing chunk 482\n",
      "Processing chunk 483\n",
      "Processing chunk 484\n",
      "Processing chunk 485\n",
      "Processing chunk 486\n",
      "Processing chunk 487\n",
      "Processing chunk 488\n",
      "Processing chunk 489\n",
      "Processing chunk 490\n",
      "Processing chunk 491\n",
      "Processing chunk 492\n",
      "Processing chunk 493\n",
      "Processing chunk 494\n",
      "Processing chunk 495\n",
      "Processing chunk 496\n",
      "Processing chunk 497\n",
      "Processing chunk 498\n",
      "Processing chunk 499\n",
      "Processing chunk 500\n",
      "Processing chunk 501\n",
      "Processing chunk 502\n",
      "Processing chunk 503\n",
      "Processing chunk 504\n",
      "Processing chunk 505\n",
      "Processing chunk 506\n",
      "Processing chunk 507\n",
      "Processing chunk 508\n",
      "Processing chunk 509\n",
      "Processing chunk 510\n",
      "Processing chunk 511\n",
      "Processing chunk 512\n",
      "Processing chunk 513\n",
      "Processing chunk 514\n",
      "Processing chunk 515\n",
      "Processing chunk 516\n",
      "Processing chunk 517\n",
      "Processing chunk 518\n",
      "Processing chunk 519\n",
      "Processing chunk 520\n",
      "Processing chunk 521\n",
      "Processing chunk 522\n",
      "Processing chunk 523\n",
      "Processing chunk 524\n",
      "Processing chunk 525\n",
      "Processing chunk 526\n",
      "Processing chunk 527\n",
      "Processing chunk 528\n",
      "Processing chunk 529\n",
      "Processing chunk 530\n",
      "Processing chunk 531\n",
      "Processing chunk 532\n",
      "Processing chunk 533\n",
      "Processing chunk 534\n",
      "Processing chunk 535\n",
      "Processing chunk 536\n",
      "Processing chunk 537\n",
      "Processing chunk 538\n",
      "Processing chunk 539\n",
      "Processing chunk 540\n",
      "Processing chunk 541\n",
      "Processing chunk 542\n",
      "Processing chunk 543\n",
      "Processing chunk 544\n",
      "Processing chunk 545\n",
      "Processing chunk 546\n",
      "Processing chunk 547\n",
      "Processing chunk 548\n",
      "Processing chunk 549\n",
      "Processing chunk 550\n",
      "Processing chunk 551\n",
      "Processing chunk 552\n",
      "Processing chunk 553\n",
      "Processing chunk 554\n",
      "Processing chunk 555\n",
      "Processing chunk 556\n",
      "Processing chunk 557\n",
      "Processing chunk 558\n",
      "Processing chunk 559\n",
      "Processing chunk 560\n",
      "Processing chunk 561\n",
      "Processing chunk 562\n",
      "Processing chunk 563\n",
      "Processing chunk 564\n",
      "Processing chunk 565\n",
      "Processing chunk 566\n",
      "Processing chunk 567\n",
      "Processing chunk 568\n",
      "Processing chunk 569\n",
      "Processing chunk 570\n",
      "Processing chunk 571\n",
      "Processing chunk 572\n",
      "Processing chunk 573\n",
      "Processing chunk 574\n",
      "Processing chunk 575\n",
      "Processing chunk 576\n",
      "Processing chunk 577\n",
      "Processing chunk 578\n",
      "Processing chunk 579\n",
      "Processing chunk 580\n",
      "Processing chunk 581\n",
      "Processing chunk 582\n",
      "Processing chunk 583\n",
      "Processing chunk 584\n",
      "Processing chunk 585\n",
      "Processing chunk 586\n",
      "Processing chunk 587\n",
      "Processing chunk 588\n",
      "Processing chunk 589\n",
      "Processing chunk 590\n",
      "Processing chunk 591\n",
      "Processing chunk 592\n",
      "Processing chunk 593\n",
      "Processing chunk 594\n",
      "Processing chunk 595\n",
      "Processing chunk 596\n",
      "Processing chunk 597\n",
      "Processing chunk 598\n",
      "Processing chunk 599\n",
      "Processing chunk 600\n",
      "Processing chunk 601\n",
      "Processing chunk 602\n",
      "Processing chunk 603\n",
      "Processing chunk 604\n",
      "Processing chunk 605\n",
      "Processing chunk 606\n",
      "Processing chunk 607\n",
      "Processing chunk 608\n",
      "Processing chunk 609\n",
      "Processing chunk 610\n",
      "Processing chunk 611\n",
      "Processing chunk 612\n",
      "Processing chunk 613\n",
      "Processing chunk 614\n",
      "Processing chunk 615\n",
      "Processing chunk 616\n",
      "Processing chunk 617\n",
      "Processing chunk 618\n",
      "Processing chunk 619\n",
      "Processing chunk 620\n",
      "Processing chunk 621\n",
      "Processing chunk 622\n",
      "Processing chunk 623\n",
      "Processing chunk 624\n",
      "Processing chunk 625\n",
      "Processing chunk 626\n",
      "Processing chunk 627\n",
      "Processing chunk 628\n",
      "Processing chunk 629\n",
      "Processing chunk 630\n",
      "Processing chunk 631\n",
      "Processing chunk 632\n",
      "Processing chunk 633\n",
      "Processing chunk 634\n",
      "Processing chunk 635\n",
      "Processing chunk 636\n",
      "Processing chunk 637\n",
      "Processing chunk 638\n",
      "Processing chunk 639\n",
      "Processing chunk 640\n",
      "Processing chunk 641\n",
      "Processing chunk 642\n",
      "Processing chunk 643\n",
      "Processing chunk 644\n",
      "Processing chunk 645\n",
      "Processing chunk 646\n",
      "Processing chunk 647\n",
      "Processing chunk 648\n",
      "Processing chunk 649\n",
      "Processing chunk 650\n",
      "Processing chunk 651\n",
      "Processing chunk 652\n",
      "Processing chunk 653\n",
      "Processing chunk 654\n",
      "Processing chunk 655\n",
      "Processing chunk 656\n",
      "Processing chunk 657\n",
      "Processing chunk 658\n",
      "Processing chunk 659\n",
      "Processing chunk 660\n",
      "Processing chunk 661\n",
      "Processing chunk 662\n",
      "Processing chunk 663\n",
      "Processing chunk 664\n",
      "Processing chunk 665\n",
      "Processing chunk 666\n",
      "Processing chunk 667\n",
      "Processing chunk 668\n",
      "Processing chunk 669\n",
      "Processing chunk 670\n",
      "Processing chunk 671\n",
      "Processing chunk 672\n",
      "Processing chunk 673\n",
      "Processing chunk 674\n",
      "Processing chunk 675\n",
      "Processing chunk 676\n",
      "Processing chunk 677\n",
      "Processing chunk 678\n",
      "Processing chunk 679\n",
      "Processing chunk 680\n",
      "Processing chunk 681\n",
      "Processing chunk 682\n",
      "Processing chunk 683\n",
      "Processing chunk 684\n",
      "Processing chunk 685\n",
      "Processing chunk 686\n",
      "Processing chunk 687\n",
      "Processing chunk 688\n",
      "Processing chunk 689\n",
      "Processing chunk 690\n",
      "Processing chunk 691\n",
      "Processing chunk 692\n",
      "Processing chunk 693\n",
      "Processing chunk 694\n",
      "Processing chunk 695\n",
      "Processing chunk 696\n",
      "Processing chunk 697\n",
      "Processing chunk 698\n",
      "Processing chunk 699\n",
      "Processing chunk 700\n",
      "Processing chunk 701\n",
      "Processing chunk 702\n",
      "Processing chunk 703\n",
      "Processing chunk 704\n",
      "Processing chunk 705\n",
      "Processing chunk 706\n",
      "Processing chunk 707\n",
      "Processing chunk 708\n",
      "Processing chunk 709\n",
      "Processing chunk 710\n",
      "Processing chunk 711\n",
      "Processing chunk 712\n",
      "Processing chunk 713\n",
      "Processing chunk 714\n",
      "Processing chunk 715\n",
      "Processing chunk 716\n",
      "Processing chunk 717\n",
      "Processing chunk 718\n",
      "Processing chunk 719\n",
      "Processing chunk 720\n",
      "Processing chunk 721\n",
      "Processing chunk 722\n",
      "Processing chunk 723\n",
      "Processing chunk 724\n",
      "Processing chunk 725\n",
      "Processing chunk 726\n",
      "Processing chunk 727\n",
      "Processing chunk 728\n",
      "Processing chunk 729\n",
      "Processing chunk 730\n",
      "Processing chunk 731\n",
      "Processing chunk 732\n",
      "Processing chunk 733\n",
      "Processing chunk 734\n",
      "Processing chunk 735\n",
      "Processing chunk 736\n",
      "Processing chunk 737\n",
      "Processing chunk 738\n",
      "Processing chunk 739\n",
      "Processing chunk 740\n",
      "Processing chunk 741\n",
      "Processing chunk 742\n",
      "Processing chunk 743\n",
      "Processing chunk 744\n",
      "Processing chunk 745\n",
      "Processing chunk 746\n",
      "Processing chunk 747\n",
      "Processing chunk 748\n",
      "Processing chunk 749\n",
      "Processing chunk 750\n",
      "Processing chunk 751\n",
      "Processing chunk 752\n",
      "Processing chunk 753\n",
      "Processing chunk 754\n",
      "Processing chunk 755\n",
      "Processing chunk 756\n",
      "Processing chunk 757\n",
      "Processing chunk 758\n",
      "Processing chunk 759\n",
      "Processing chunk 760\n",
      "Processing chunk 761\n",
      "Processing chunk 762\n",
      "Processing chunk 763\n",
      "Processing chunk 764\n",
      "Processing chunk 765\n",
      "Processing chunk 766\n",
      "Processing chunk 767\n",
      "Processing chunk 768\n",
      "Processing chunk 769\n",
      "Processing chunk 770\n",
      "Processing chunk 771\n",
      "Processing chunk 772\n",
      "Processing chunk 773\n",
      "Processing chunk 774\n",
      "Processing chunk 775\n",
      "Processing chunk 776\n",
      "Processing chunk 777\n",
      "Processing chunk 778\n",
      "Processing chunk 779\n",
      "Processing chunk 780\n",
      "Processing chunk 781\n",
      "Processing chunk 782\n",
      "Processing chunk 783\n",
      "Processing chunk 784\n",
      "Processing chunk 785\n",
      "Processing chunk 786\n",
      "Processing chunk 787\n",
      "Processing chunk 788\n",
      "Processing chunk 789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 790\n",
      "Processing chunk 791\n",
      "Processing chunk 792\n",
      "Processing chunk 793\n",
      "Processing chunk 794\n",
      "Processing chunk 795\n",
      "Processing chunk 796\n",
      "Processing chunk 797\n",
      "Processing chunk 798\n",
      "Processing chunk 799\n",
      "Processing chunk 800\n",
      "Processing chunk 801\n",
      "Processing chunk 802\n",
      "Processing chunk 803\n",
      "Processing chunk 804\n",
      "Processing chunk 805\n",
      "Processing chunk 806\n",
      "Processing chunk 807\n",
      "Processing chunk 808\n",
      "Processing chunk 809\n",
      "Processing chunk 810\n",
      "Processing chunk 811\n",
      "Processing chunk 812\n",
      "Processing chunk 813\n",
      "Processing chunk 814\n",
      "Processing chunk 815\n",
      "Processing chunk 816\n",
      "Processing chunk 817\n",
      "Processing chunk 818\n",
      "Processing chunk 819\n",
      "Processing chunk 820\n",
      "Processing chunk 821\n",
      "Processing chunk 822\n",
      "Processing chunk 823\n",
      "Processing chunk 824\n",
      "Processing chunk 825\n",
      "Processing chunk 826\n",
      "Processing chunk 827\n",
      "Processing chunk 828\n",
      "Processing chunk 829\n",
      "Processing chunk 830\n",
      "Processing chunk 831\n",
      "Processing chunk 832\n",
      "Processing chunk 833\n",
      "Processing chunk 834\n",
      "Processing chunk 835\n",
      "Processing chunk 836\n",
      "Processing chunk 837\n",
      "Processing chunk 838\n",
      "Processing chunk 839\n",
      "Processing chunk 840\n",
      "Processing chunk 841\n",
      "Processing chunk 842\n",
      "Processing chunk 843\n",
      "Processing chunk 844\n",
      "Processing chunk 845\n",
      "Processing chunk 846\n",
      "Processing chunk 847\n",
      "Processing chunk 848\n",
      "Processing chunk 849\n",
      "Processing chunk 850\n",
      "Processing chunk 851\n",
      "Processing chunk 852\n",
      "Processing chunk 853\n",
      "Processing chunk 854\n",
      "Processing chunk 855\n",
      "Processing chunk 856\n",
      "Processing chunk 857\n",
      "Processing chunk 858\n",
      "Processing chunk 859\n",
      "Processing chunk 860\n",
      "Processing chunk 861\n",
      "Processing chunk 862\n",
      "Processing chunk 863\n",
      "Processing chunk 864\n",
      "Processing chunk 865\n",
      "Processing chunk 866\n",
      "Processing chunk 867\n",
      "Processing chunk 868\n",
      "Processing chunk 869\n",
      "Processing chunk 870\n",
      "Processing chunk 871\n",
      "Processing chunk 872\n",
      "Processing chunk 873\n",
      "Processing chunk 874\n",
      "Processing chunk 875\n",
      "Processing chunk 876\n",
      "Processing chunk 877\n",
      "Processing chunk 878\n",
      "Processing chunk 879\n",
      "Processing chunk 880\n",
      "Processing chunk 881\n",
      "Processing chunk 882\n",
      "Processing chunk 883\n",
      "Processing chunk 884\n",
      "Processing chunk 885\n",
      "Processing chunk 886\n",
      "Processing chunk 887\n",
      "Processing chunk 888\n",
      "Processing chunk 889\n",
      "Processing chunk 890\n",
      "Processing chunk 891\n",
      "Processing chunk 892\n",
      "Processing chunk 893\n",
      "Processing chunk 894\n",
      "Processing chunk 895\n",
      "Processing chunk 896\n",
      "Processing chunk 897\n",
      "Processing chunk 898\n",
      "Processing chunk 899\n",
      "Processing chunk 900\n",
      "Processing chunk 901\n",
      "Processing chunk 902\n",
      "Processing chunk 903\n",
      "Processing chunk 904\n",
      "Processing chunk 905\n",
      "Processing chunk 906\n",
      "Processing chunk 907\n",
      "Processing chunk 908\n",
      "Processing chunk 909\n",
      "Processing chunk 910\n",
      "Processing chunk 911\n",
      "Processing chunk 912\n",
      "Processing chunk 913\n",
      "Processing chunk 914\n",
      "Processing chunk 915\n",
      "Processing chunk 916\n",
      "Processing chunk 917\n",
      "Processing chunk 918\n",
      "Processing chunk 919\n",
      "Processing chunk 920\n",
      "Processing chunk 921\n",
      "Processing chunk 922\n",
      "Processing chunk 923\n",
      "Processing chunk 924\n",
      "Processing chunk 925\n",
      "Processing chunk 926\n",
      "Processing chunk 927\n",
      "Processing chunk 928\n",
      "Processing chunk 929\n",
      "Processing chunk 930\n",
      "Processing chunk 931\n",
      "Processing chunk 932\n",
      "Processing chunk 933\n",
      "Processing chunk 934\n",
      "Processing chunk 935\n",
      "Processing chunk 936\n",
      "Processing chunk 937\n",
      "Processing chunk 938\n",
      "Processing chunk 939\n",
      "Processing chunk 940\n",
      "Processing chunk 941\n",
      "Processing chunk 942\n",
      "Processing chunk 943\n",
      "Processing chunk 944\n",
      "Processing chunk 945\n",
      "Processing chunk 946\n",
      "Processing chunk 947\n",
      "Processing chunk 948\n",
      "Processing chunk 949\n",
      "Processing chunk 950\n",
      "Processing chunk 951\n",
      "Processing chunk 952\n",
      "Processing chunk 953\n",
      "Processing chunk 954\n",
      "Processing chunk 955\n",
      "Processing chunk 956\n",
      "Processing chunk 957\n",
      "Processing chunk 958\n",
      "Processing chunk 959\n",
      "Processing chunk 960\n",
      "Processing chunk 961\n",
      "Processing chunk 962\n",
      "Processing chunk 963\n",
      "Processing chunk 964\n",
      "Processing chunk 965\n",
      "Processing chunk 966\n",
      "Processing chunk 967\n",
      "Processing chunk 968\n",
      "Processing chunk 969\n",
      "Processing chunk 970\n",
      "Processing chunk 971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m parquet_writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(io\u001b[38;5;241m.\u001b[39mStringIO(csv_content), chunksize\u001b[38;5;241m=\u001b[39mchunk_size, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1698\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow)\n\u001b[0;32m-> 1810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1068\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your existing code\n",
    "chunk_size = 10000  # Adjust the chunk size based on available memory\n",
    "parquet_buffer = io.BytesIO()\n",
    "\n",
    "parquet_writer = None\n",
    "schema = None\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(io.StringIO(csv_content), chunksize=chunk_size, dtype=str)):\n",
    "    print(f\"Processing chunk {i+1}\")\n",
    "    chunk = chunk.fillna(\"N/A\")\n",
    "    if i == 0:\n",
    "        schema = pa.Schema.from_pandas(df=chunk)\n",
    "        parquet_writer = pq.ParquetWriter(parquet_buffer, schema, compression='gzip')\n",
    "    table = pa.Table.from_pandas(chunk, schema=schema)\n",
    "    parquet_writer.write_table(table)\n",
    "\n",
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "\n",
    "parquet_buffer.seek(0)\n",
    "\n",
    "# Save the Parquet file\n",
    "with open(\"output.parquet\", \"wb\") as f:\n",
    "    f.write(parquet_buffer.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7719dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE_PATH=\"Parking_Violations_Issued_Fiscal_Year_2024.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2b2592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summons Number</th>\n",
       "      <th>Plate ID</th>\n",
       "      <th>Registration State</th>\n",
       "      <th>Plate Type</th>\n",
       "      <th>Issue Date</th>\n",
       "      <th>Violation Code</th>\n",
       "      <th>Vehicle Body Type</th>\n",
       "      <th>Vehicle Make</th>\n",
       "      <th>Issuing Agency</th>\n",
       "      <th>Street Code1</th>\n",
       "      <th>...</th>\n",
       "      <th>Vehicle Color</th>\n",
       "      <th>Unregistered Vehicle?</th>\n",
       "      <th>Vehicle Year</th>\n",
       "      <th>Meter Number</th>\n",
       "      <th>Feet From Curb</th>\n",
       "      <th>Violation Post Code</th>\n",
       "      <th>Violation Description</th>\n",
       "      <th>No Standing or Stopping Violation</th>\n",
       "      <th>Hydrant Violation</th>\n",
       "      <th>Double Parking Violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1159637337</td>\n",
       "      <td>KZH2758</td>\n",
       "      <td>NY</td>\n",
       "      <td>PAS</td>\n",
       "      <td>06/09/2023</td>\n",
       "      <td>67</td>\n",
       "      <td>VAN</td>\n",
       "      <td>HONDA</td>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>BLUE</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252960645</td>\n",
       "      <td>JPD8746</td>\n",
       "      <td>NY</td>\n",
       "      <td>PAS</td>\n",
       "      <td>06/30/2023</td>\n",
       "      <td>87</td>\n",
       "      <td>SUBN</td>\n",
       "      <td>LINCO</td>\n",
       "      <td>M</td>\n",
       "      <td>17870</td>\n",
       "      <td>...</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1252960669</td>\n",
       "      <td>JPD8746</td>\n",
       "      <td>NY</td>\n",
       "      <td>PAS</td>\n",
       "      <td>06/30/2023</td>\n",
       "      <td>31</td>\n",
       "      <td>SUBN</td>\n",
       "      <td>LINCO</td>\n",
       "      <td>M</td>\n",
       "      <td>17870</td>\n",
       "      <td>...</td>\n",
       "      <td>GRAY</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1252994126</td>\n",
       "      <td>MBH9245</td>\n",
       "      <td>99</td>\n",
       "      <td>PAS</td>\n",
       "      <td>07/06/2023</td>\n",
       "      <td>20</td>\n",
       "      <td>SDN</td>\n",
       "      <td>KIA</td>\n",
       "      <td>M</td>\n",
       "      <td>12690</td>\n",
       "      <td>...</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1252994175</td>\n",
       "      <td>MBH9245</td>\n",
       "      <td>PA</td>\n",
       "      <td>PAS</td>\n",
       "      <td>07/08/2023</td>\n",
       "      <td>40</td>\n",
       "      <td>SDN</td>\n",
       "      <td>KIA</td>\n",
       "      <td>M</td>\n",
       "      <td>12690</td>\n",
       "      <td>...</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Summons Number Plate ID Registration State Plate Type  Issue Date  \\\n",
       "0     1159637337  KZH2758                 NY        PAS  06/09/2023   \n",
       "1     1252960645  JPD8746                 NY        PAS  06/30/2023   \n",
       "2     1252960669  JPD8746                 NY        PAS  06/30/2023   \n",
       "3     1252994126  MBH9245                 99        PAS  07/06/2023   \n",
       "4     1252994175  MBH9245                 PA        PAS  07/08/2023   \n",
       "\n",
       "  Violation Code Vehicle Body Type Vehicle Make Issuing Agency Street Code1  \\\n",
       "0             67               VAN        HONDA              P            0   \n",
       "1             87              SUBN        LINCO              M        17870   \n",
       "2             31              SUBN        LINCO              M        17870   \n",
       "3             20               SDN          KIA              M        12690   \n",
       "4             40               SDN          KIA              M        12690   \n",
       "\n",
       "   ... Vehicle Color Unregistered Vehicle? Vehicle Year Meter Number  \\\n",
       "0  ...          BLUE                     0         2006            -   \n",
       "1  ...          GRAY                     0         2020            -   \n",
       "2  ...          GRAY                     0         2020            -   \n",
       "3  ...         WHITE                     0            0            -   \n",
       "4  ...         WHITE                     0            0            -   \n",
       "\n",
       "  Feet From Curb Violation Post Code Violation Description  \\\n",
       "0              0                 N/A                   N/A   \n",
       "1              0                 N/A                   N/A   \n",
       "2              0                 N/A                   N/A   \n",
       "3              0                 N/A                   N/A   \n",
       "4              0                 N/A                   N/A   \n",
       "\n",
       "  No Standing or Stopping Violation Hydrant Violation Double Parking Violation  \n",
       "0                               N/A               N/A                      N/A  \n",
       "1                               N/A               N/A                      N/A  \n",
       "2                               N/A               N/A                      N/A  \n",
       "3                               N/A               N/A                      N/A  \n",
       "4                               N/A               N/A                      N/A  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f54ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_cols = df.select_dtypes(include=['object']).columns\n",
    "# df[obj_cols] = df[obj_cols].astype(str)\n",
    "\n",
    "# int_cols = df.select_dtypes(include=['int64']).columns\n",
    "# df[int_cols] = df[int_cols].astype('int32')\n",
    "\n",
    "# float_cols = df.select_dtypes(include=['float64']).columns\n",
    "# df[float_cols] = df[float_cols].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5370330c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kz/pf1fvjvx2dl42wjkh0j61znm0000gn/T/ipykernel_10166/4120585546.py:9: DtypeWarning: Columns (17,18,20,22,23,29,30,31,32,36,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Summons Number Plate ID Registration State Plate Type  Issue Date  \\\n",
      "0      1159637337  KZH2758                 NY        PAS  06/09/2023   \n",
      "1      1252960645  JPD8746                 NY        PAS  06/30/2023   \n",
      "2      1252960669  JPD8746                 NY        PAS  06/30/2023   \n",
      "3      1252994126  MBH9245                 99        PAS  07/06/2023   \n",
      "4      1252994175  MBH9245                 PA        PAS  07/08/2023   \n",
      "\n",
      "   Violation Code Vehicle Body Type Vehicle Make Issuing Agency  Street Code1  \\\n",
      "0              67               VAN        HONDA              P             0   \n",
      "1              87              SUBN        LINCO              M         17870   \n",
      "2              31              SUBN        LINCO              M         17870   \n",
      "3              20               SDN          KIA              M         12690   \n",
      "4              40               SDN          KIA              M         12690   \n",
      "\n",
      "   ...  Vehicle Color  Unregistered Vehicle?  Vehicle Year  Meter Number  \\\n",
      "0  ...           BLUE                    0.0          2006             -   \n",
      "1  ...           GRAY                    0.0          2020             -   \n",
      "2  ...           GRAY                    0.0          2020             -   \n",
      "3  ...          WHITE                    0.0             0             -   \n",
      "4  ...          WHITE                    0.0             0             -   \n",
      "\n",
      "   Feet From Curb  Violation Post Code  Violation Description  \\\n",
      "0               0                  NaN                    NaN   \n",
      "1               0                  NaN                    NaN   \n",
      "2               0                  NaN                    NaN   \n",
      "3               0                  NaN                    NaN   \n",
      "4               0                  NaN                    NaN   \n",
      "\n",
      "  No Standing or Stopping Violation Hydrant Violation Double Parking Violation  \n",
      "0                               NaN               NaN                      NaN  \n",
      "1                               NaN               NaN                      NaN  \n",
      "2                               NaN               NaN                      NaN  \n",
      "3                               NaN               NaN                      NaN  \n",
      "4                               NaN               NaN                      NaN  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "# Path to the gzipped CSV file\n",
    "file_path = 'Parking_Violations_Issued_Fiscal_Year_2024.csv.gz'\n",
    "\n",
    "# Read the gzipped CSV file\n",
    "with gzip.open(file_path, 'rt') as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8563344",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Get the current working directory\n",
    "    \n",
    "    # print(\"beforecwd\",os.getcwd())\n",
    "    # print(\"files\",os.listdir(os.getcwd()))\n",
    "\n",
    "\n",
    "    # os.chdir('/opt/airflow/data')\n",
    "\n",
    "    # cwd = os.getcwd()\n",
    "\n",
    "    # files = os.listdir(cwd)\n",
    "\n",
    "    # print(\"cwd\",cwd)\n",
    "    # print(\"files\",files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow.csv as pacsv\n",
    "\n",
    "# # Polars\n",
    "# def polars():\n",
    "#     start_time = time.time()\n",
    "#     polars_time = time.time() - start_time\n",
    "#     df = pl.scan_csv(csv_file)\n",
    "#     polars_time = time.time() - start_time\n",
    "#     # Convert to Parquet\n",
    "#     start_time = time.time()\n",
    "#     df.sink_parquet(\n",
    "#         parquet_file,\n",
    "#         compression=\"snappy\",\n",
    "#         row_group_size=100_000\n",
    "#     )\n",
    "\n",
    "#     #df = None\n",
    "#     polars_parquet_time = time.time() - start_time\n",
    "\n",
    "#     print(f\"Polars time: {polars_time:.2f} seconds\")\n",
    "#     print(f\"Polars to Parquet time: {polars_parquet_time:.2f} seconds\")\n",
    "\n",
    "# # Pyarrow\n",
    "# csv_file=\"Parking_Violations_Issued_-_Fiscal_Year_2024.csv\"\n",
    "# parquet_file=\"Parking_Violations_Issued_Fiscal_Year_2024_polars.parquet\"\n",
    "# start_time = time.time()\n",
    "# parse_options = pacsv.ParseOptions(delimiter=\"\\t\", quote_char=\"^\")\n",
    "# df = pacsv.read_csv(csv_file, parse_options=parse_options)\n",
    "\n",
    "# pyarrow_time = time.time() - start_time\n",
    "\n",
    "# # Convert to Parquet\n",
    "# start_time = time.time()\n",
    "# pq.write_table(df, parquet_file)\n",
    "# pyarrow_parquet_time = time.time() - start_time\n",
    "# table_pyarrow = None\n",
    "# print(f\"Pyarrow time: {pyarrow_time:.2f} seconds\")\n",
    "# print(f\"Pyarrow to Parquet time: {pyarrow_parquet_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4829c7",
   "metadata": {},
   "source": [
    "## Not Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3eee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get County_Precinct Data from here (https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3678656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct data for the precincts with matching lengths for County and Precinct arrays\n",
    "data = {\n",
    "    \"County\": [\"Manhattan\"] * 22 + [\"Bronx\"] * 12 + [\"Brooklyn\"] * 23 + [\"Queens\"] * 16 + [\"Staten Island\"] * 4,\n",
    "    \"Precinct\": [\n",
    "        \"1st Precinct\", \"5th Precinct\", \"6th Precinct\", \"7th Precinct\", \"9th Precinct\", \"10th Precinct\", \"13th Precinct\", \n",
    "        \"Midtown South Precinct\", \"17th Precinct\", \"Midtown North Precinct\", \"19th Precinct\", \"20th Precinct\", \n",
    "        \"Central Park Precinct\", \"23rd Precinct\", \"24th Precinct\", \"25th Precinct\", \"26th Precinct\", \"28th Precinct\", \n",
    "        \"30th Precinct\", \"32nd Precinct\", \"33rd Precinct\", \"34th Precinct\", \n",
    "        \"40th Precinct\", \"41st Precinct\", \"42nd Precinct\", \"43rd Precinct\", \"44th Precinct\", \"45th Precinct\", \n",
    "        \"46th Precinct\", \"47th Precinct\", \"48th Precinct\", \"49th Precinct\", \"50th Precinct\", \"52nd Precinct\", \n",
    "        \"60th Precinct\", \"61st Precinct\", \"62nd Precinct\", \"63rd Precinct\", \"66th Precinct\", \"67th Precinct\", \n",
    "        \"68th Precinct\", \"69th Precinct\", \"70th Precinct\", \"71st Precinct\", \"72nd Precinct\", \"73rd Precinct\", \n",
    "        \"75th Precinct\", \"76th Precinct\", \"77th Precinct\", \"78th Precinct\", \"79th Precinct\", \"81st Precinct\", \n",
    "        \"83rd Precinct\", \"84th Precinct\", \"88th Precinct\", \"90th Precinct\", \"94th Precinct\", \n",
    "        \"100th Precinct\", \"101st Precinct\", \"102nd Precinct\", \"103rd Precinct\", \"104th Precinct\", \"105th Precinct\", \n",
    "        \"106th Precinct\", \"107th Precinct\", \"108th Precinct\", \"109th Precinct\", \"110th Precinct\", \"111th Precinct\", \n",
    "        \"112th Precinct\", \"113th Precinct\", \"114th Precinct\", \"115th Precinct\", \n",
    "        \"120th Precinct\", \"121st Precinct\", \"122nd Precinct\", \"123rd Precinct\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating the dataframe\n",
    "df_precincts = pd.DataFrame(data)\n",
    "\n",
    "# Extracting the precinct numbers\n",
    "df_precincts[\"Precinct Number\"] = df_precincts[\"Precinct\"].str.extract(r'(\\d+)', expand=False)\n",
    "\n",
    "# Dropping rows where precinct number extraction failed\n",
    "df_precincts = df_precincts.dropna(subset=[\"Precinct Number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8800d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precincts.to_csv(\"County_Precinct.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9d44f",
   "metadata": {},
   "source": [
    "## Chunking code (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23d70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task to convert CSV to Parquet directly in S3\n",
    "def convert_csv_to_parquet():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting the CSV to Parquet conversion process\")\n",
    "\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "\n",
    "    # Download gzipped CSV file content from S3\n",
    "    print(f\"Downloading gzipped CSV content from S3 bucket {S3_BUCKET}, key {CSV_FILE_PATH_VIOLATION}\")\n",
    "    gzipped_csv_content = s3_hook.get_key(key=CSV_FILE_PATH_VIOLATION, bucket_name=S3_BUCKET).get()[\"Body\"].read()\n",
    "    \n",
    "    # Decompress gzipped CSV content\n",
    "    print(\"Decompressing gzipped CSV content\")\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(gzipped_csv_content), mode='rb') as gz:\n",
    "        csv_content = gz.read().decode('utf-8')\n",
    "\n",
    "    # Read CSV content into DataFrame in chunks\n",
    "    print(\"Reading CSV content into DataFrame in chunks\")\n",
    "    chunk_size = 5000\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    \n",
    "    parquet_writer = None\n",
    "    schema = None\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(io.StringIO(csv_content), chunksize=chunk_size, dtype=str)):\n",
    "        print(f\"Processing chunk {i+1}\")\n",
    "        \n",
    "        # Handle null values and adjust column names\n",
    "        chunk = chunk.fillna(\"N/A\")\n",
    "        chunk.columns = [col.upper().replace(' ', '_') for col in chunk.columns]\n",
    "        chunk.columns = [col.upper().replace('?', '') for col in chunk.columns]\n",
    "\n",
    "        if i == 0:\n",
    "            schema = pa.Schema.from_pandas(df=chunk)\n",
    "            parquet_writer = pq.ParquetWriter(parquet_buffer, schema, compression='gzip')\n",
    "        table = pa.Table.from_pandas(chunk, schema=schema)\n",
    "        parquet_writer.write_table(table)\n",
    "    \n",
    "    if parquet_writer:\n",
    "        parquet_writer.close()\n",
    "    \n",
    "    parquet_buffer.seek(0)\n",
    "    print(\"Parquet file created in memory\")\n",
    "\n",
    "    # Upload Parquet file content to S3\n",
    "    print(f\"Uploading Parquet file to S3 bucket {S3_BUCKET}, key {PARQUET_FILE_PATH_VIOLATION}\")\n",
    "    s3_hook.load_bytes(parquet_buffer.read(), key=PARQUET_FILE_PATH_VIOLATION, bucket_name=S3_BUCKET, replace=True)\n",
    "    print(f\"Uploaded Parquet file to S3 bucket {S3_BUCKET}\")\n",
    "\n",
    "    # Remove files from S3\n",
    "    s3_hook.delete_objects(bucket=S3_BUCKET, keys=CSV_FILE_PATH_VIOLATION)\n",
    "    print(f\"Files {CSV_FILE_PATH_VIOLATION} removed from S3 bucket {S3_BUCKET}.\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = (end_time - start_time) / 60\n",
    "    print(f\"CSV to Parquet conversion completed in {duration:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17b117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to estimate bytes per line\n",
    "def estimate_bpl(file_path, n_rows=10):\n",
    "    \"\"\"Return estimates of bytes per line using the first n lines\"\"\"\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    gzipped_csv_content = s3_hook.get_key(key=file_path, bucket_name=S3_BUCKET).get()[\"Body\"].read()\n",
    "\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(gzipped_csv_content), mode='rb') as gz:\n",
    "        csv_content = gz.read().decode('utf-8')\n",
    "\n",
    "    lines = csv_content.splitlines()\n",
    "    total_length = sum(len(line.encode('utf-8')) for line in lines[:n_rows])\n",
    "    return total_length / n_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate chunk size based on available memory\n",
    "print(\"Estimating chunk size based on available memory\")\n",
    "fill_rate = 0.1\n",
    "avail_mem = psutil.virtual_memory().available\n",
    "bpl = estimate_bpl(CSV_FILE_PATH_VIOLATION)\n",
    "chunk_size = int(avail_mem * fill_rate / bpl)\n",
    "print(f\"Estimated chunk size: {chunk_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
